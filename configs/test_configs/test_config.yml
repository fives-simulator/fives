---

# Test config for a tiny Lustre filesystem
# - 1 node with 3 OST
# - 1 node with 5 OST
# - Small compute system on the side

general:
  config_name: StorAlloc_Test_Lustre_HetOss
  config_version: 0.0.1
  max_stripe_size: 100000000          # Max size for allocation stripes (in bytes) (when striping is done at CSS level, so not used here)
  walltime_extension: 1.2             # The final walltime passed to the batch sched will be walltime * walltime_extension
  debug: False
  testing: False
network:
  bandwidth_backbone: 9TBps
  bandwidth_backbone_storage: 25000MBps
  bandwidth_backbone_perm_storage: 90GBps
  bandwidth_backbone_ctrl: 12.5GBps
  link_latency: 24us
storage:
  read_variability: 1.0             # read AND write set to 1.0 to deactivate
  write_variability: 1.0
  non_linear_coef_read: 0.9         # read AND write set to 1.0 to deactivate
  non_linear_coef_write: 0.6
  static_read_overhead_seconds: 0
  static_write_overhead_seconds: 0
  io_buffer_size: 100MB
  read_bytes_preload_thres: 100000000
  write_bytes_copy_thres:  2000000000
  cleanup_threshold: 0.0
  read_nodes_inflection_param: 10
  read_nodes_rate_param: 0.001
  write_nodes_inflection_param: 10 
  write_nodes_rate_param: 0.001
  max_files: 50
  read_files_inflection_param: 10
  read_files_rate_param: 0.002
  write_files_inflection_param: 10
  write_files_rate_param: 0.002
  disk_templates:
    - &raid_OSS
      id: raid_OSS
      capacity: 20000    # GB
      read_bw: 1000    # MBps
      write_bw: 500    # MBps
      mount_prefix: /dev/hdd
  node_templates:
    - &node_capa_A
      id: lustre_OSS_A
      disks:
        - quantity: 3
          template: *raid_OSS
    - &node_capa_B
      id: lustre_OSS_B
      disks:
        - quantity: 5
          template: *raid_OSS
  nodes:
    - quantity: 1
      template: *node_capa_A
    - quantity: 1
      template: *node_capa_B
permanent_storage:
  read_bw: 10000MBps   # ALCF Tape archive cache speed is 90Gbps / and also the link between HDR and FDR is 100GBps. Here we reduce the link speed
  write_bw: 10000MBps  # Same
  capacity: 10000TB  # Tape capacity is 305 PB
  mount_prefix: /dev/disk0
  read_path: read/
  write_path: write/
  disk_id: perm_disk0
  io_buffer_size: 1GB
dragonfly:          # See https://simgrid.org/doc/latest/Platform_examples.html - Dragonfly
  groups: 2        # Entire Dragonfly zone (should be 4392 nodes in total, we use slightly more)
  group_links: 2
  chassis: 2        # Per group
  chassis_links: 2
  routers: 2       # Per chassis
  router_links: 2
  nodes: 4          # Per router
  core_count: 64    # Compute node core count - unused
  ram: 192          # GB Compute node ram
  flops: 2.2Tf      # Flops for each compute node
  max_compute_nodes: 4392
  node_local_storage:
    enabled: no     # Not implemented yet anyway
    nb_disks: 1
    capacity: 128 # GB
    read_bw: 1000 # MBps      # https://docs.alcf.anl.gov/data-management/filesystem-and-storage/file-systems/
    write_bw: 500 # MBps      # https://www.alcf.anl.gov/sites/default/files/2022-08/CompPerfWorkshop_May2022_IOopt_mcpheeters.pdf
allocator: lustre
lustre:
  lq_threshold_rr: 43
  lq_prio_free: 232
  max_nb_ost: 2000
  stripe_size: 40000000  # 40MB
  max_chunks_per_ost: 80
  min_stripe_count: 1
  read_sc_inflection_param: 10
  read_sc_rate_param: 0.001
  write_sc_inflection_param: 10
  write_sc_rate_param: 0.001
outputs:
  job_filename_prefix: simulatedJobs_
  io_actions_prefix: io_actions_ts_
  storage_svc_prefix: storage_services_operations_
...
