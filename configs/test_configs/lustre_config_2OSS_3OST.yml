---

# Test config for a tiny Lustre filesystem
# - 3 HDD per node
# - 2 nodes (in total)
# - We used two different node templates with the same caracteristics, but different IDs in order to test more easily.
# - Small compute system on the side

general:
  backbone_bw: 25000MBps    # 200Gbps backbone...
  config_version: 0.0.1
  config_name: StorAlloc_Test_Lustre_2OSSTypes
  max_stripe_size: 640000000    # Max size for allocation stripes (in bytes) (when striping is done at CSS level)
  permanent_storage_read_bw: 10000MBps
  permanent_storage_write_bw: 10000MBps
  permanent_storage_capacity: 10000TB
  preload_percent: 0
  amdahl: 0.5
  walltime_extension: 1.2             # The final walltime passed to the batch sched will be walltime * walltime_extension
  non_linear_coef_read: 0.9
  non_linear_coef_write: 0.6
  read_variability: 1.0
  write_variability: 0.8
  nb_files_per_read: 2
  nb_files_per_write: 2
  io_read_node_ratio: 0.1
  io_write_node_ratio: 0.1
storage:
  disk_templates:
    - &raid_OSS
      id: raid_OSS
      capacity: 20000    # GB
      read_bw: 1000    # MBps
      write_bw: 500    # MBps
      mount_prefix: /dev/hdd
  node_templates:
    - &node_capa_A
      id: lustre_OSS_A
      disks:
        - quantity: 3
          template: *raid_OSS
    - &node_capa_B
      id: lustre_OSS_B
      disks:
        - quantity: 3
          template: *raid_OSS
  nodes:
    - quantity: 1
      template: *node_capa_A
    - quantity: 1
      template: *node_capa_B
dragonfly:
  groups: 2         # Entire Dragonfly zone
  group_links: 2
  chassis: 2        # per group
  chassis_links: 2
  routers: 2        # per chassis
  router_links: 2
  nodes: 2          # per router
  core_count: 64    # Compute node core count - unused
  ram: 192GB        # Compute node ram - unused
  node_local_storage:
    enabled: no     # Not implemented yet anyway
    nb_disks: 1
    capacity: 8 # GB
    read_bw: 1000 # MBps
    write_bw: 500 # MBps
allocator: lustre
lustre:
  lq_threshold_rr: 43     # lustre default
  lq_prio_free: 232       # lustre default
  stripe_size: 40000000    
  stripe_count: 8         
  max_chunks_per_ost: 80  # Maximum number of file parts / allocation chunks on each OSTs (not a Lustre parameter, but used to speed up simulation at the cost of precision)
...
