---

# Config for a look-alike Theta@ALCF system
# - 41 HDD per node (6TB each) - configured into a distributed RAID (modeled as a single disk)
# - 1 OST per OSS, 56 OSS (considering one OSS per node)
# - Total of 9.2 PB in Lustre (theta-fs0) with agg bw ~ 240GBps
# Compute side has 4392 nodes in a Dragonfly topo (we have slightly more in the end, at ~4600 nodes)

general:
  config_name: Fives_BLuewaters
  config_version: 0.1.0
  max_stripe_size: 100048576          # Max size for allocation stripes (in bytes) (when striping is done at CSS level, so not used here)
  preload_percent: 0.0                # DEPRECATED
  walltime_extension: 1.0             # The final walltime passed to the batch sched will be walltime * walltime_extension
network:
  bandwidth_backbone: 9TBps
  bandwidth_backbone_storage: 200GBps     # 240Gbps theoretical
  bandwidth_backbone_perm_storage: 80GBps # 90Gbps theoretical
  bandwidth_backbone_ctrl: 12.5GBps
  link_latency: 24us
storage:
  read_variability: 1.0               # read AND write set to 1.0 to deactivate
  write_variability: 1.0
  non_linear_coef_read: 0.85          # read AND write set to 1.0 to deactivate
  non_linear_coef_write: 0.75
  read_node_param: 0.5           # calibrated
  write_node_param: 0.5          # calibrated
  io_buffer_size: 536870912B
  read_bytes_preload_thres: 1000000000 # Not doing copy *in* if job reads more than 1GB
  write_bytes_copy_thres:   2000000000 # Not doing copy *out* if job writes more than 2GB
  link_bw: 10e9  # b/s
  disk_templates:
    - &raid_OST
      id: raid_OST
      capacity: 16000    # GB 
      read_bw: 4440       # MBps   theoretical 6GBps peak performance (https://www.alcf.anl.gov/sites/default/files/2022-08/CompPerfWorkshop_May2022_IOopt_mcpheeters.pdf)
      write_bw: 4120      # MBps 
      mount_prefix: /dev/mdraid
  node_templates:
    - &node_OSS
      id: lustre_OSS
      disks:
        - quantity: 4
          template: *raid_OST
  nodes:
    - quantity: 360
      template: *node_OSS
permanent_storage:  # Using ~ characteristics of the project/ lustre (10x smaller than scratch)
  read_bw: 100GBps        # we consider this storage to have little effect on 
  write_bw: 100GBps  
  capacity: 2000TB  # Tape capacity is 305 PB
  mount_prefix: /dev/project
  read_path: read
  write_path: write
  disk_id: perm_disk0
  io_buffer_size: 536870912B
torus:
  core_count: 64
  flops: 300Gf
  max_compute_nodes: 13432
  ram: 32
  link_bw: 10e9  # b/s
allocator: lustre
lustre:
  lq_threshold_rr: 43     # lustre default
  lq_prio_free: 232       # lustre default
  max_nb_ost: 2000        # lustre limit
  stripe_size: 268435456  # Default size of each read / write before switching to another stripe on a different OST (from https://www.alcf.anl.gov/sites/default/files/2022-08/CompPerfWorkshopq_May2022_IOopt_mcpheeters.pdf)
  max_chunks_per_ost: 22
  stripe_count: 1
  stripe_count_high_read_add: 1
  stripe_count_high_thresh_read: 231004494
  stripe_count_high_thresh_write: 0
  stripe_count_high_write_add: 1
outputs:
  job_filename_prefix: simulatedJobs_
  io_actions_prefix: io_actions_ts_
  storage_svc_prefix: storage_services_operations_
...
