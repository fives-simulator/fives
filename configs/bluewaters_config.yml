---

general:
  config_name: Fives_BlueWaters_Lustre_scratch
  config_version: 0.0.1
  max_stripe_size: 100048576          # Max size for allocation stripes (in bytes) (when striping is done at CSS level, so not used here)
  preload_percent: 0.0
  walltime_extension: 1.0             # The final walltime passed to the batch sched will be walltime * walltime_extension
network:
  bandwidth_backbone: 9TBps
  bandwidth_backbone_storage: 200GBps     # 240Gbps theoretical
  bandwidth_backbone_perm_storage: 80GBps # 90Gbps theoretical
  bandwidth_backbone_ctrl: 12.5GBps
  link_latency: 24us
storage:
  read_variability: 1.0               # read AND write set to 1.0 to deactivate
  write_variability: 1.0
  non_linear_coef_read: 0.85          # read AND write set to 1.0 to deactivate
  non_linear_coef_write: 0.75
  nb_files_per_read: 2
  nb_files_per_write: 2
  read_node_thres: 50000000
  write_node_thres: 40000000
  static_read_overhead_seconds: 0
  static_write_overhead_seconds: 0
  io_buffer_size: 536870912B
  read_bytes_preload_thres: 1000000000 # Not doing copy *in* if job reads more than 1GB
  write_bytes_copy_thres:   2000000000 # Not doing copy *out* if job writes more than 2GB
  cleanup_threshold: 0.75
  disk_templates:
    - &raid_OSS
      id: raid_OSS
      capacity: 16000    # GB (~ 16 TB per RAID (8 disks + 2))
      read_bw: 2500       # MBps
      write_bw: 1400      # MBps 
      mount_prefix: /dev/hdd
  node_templates:
    - &node_capa
      id: lustre_OSS
      disks:
        - quantity: 4
          template: *raid_OSS
  nodes:
    - quantity: 360        # 360 OSTs
      template: *node_capa
permanent_storage:
  read_bw: 20GBps   # ALCF Tape archive cache speed is 90Gbps / and also the link between HDR and FDR is 100GBps
  write_bw: 10GBps  # Same
  capacity: 3050TB  # Tape capacity is 305 PB
  mount_prefix: /dev/disk0
  read_path: read/
  write_path: write/
  disk_id: perm_disk0
  io_buffer_size: 536870912B
torus:          # See https://simgrid.org/doc/latest/Platform_examples.html - Dragonfly
  core_count: 64    # Compute node core count - unused
  ram: 32          # GB Compute node ram
  flops: 300Gf      # Flops for each compute node
  max_compute_nodes: 13432 # there should be 2 nodes per Gemini router in the Blue Waters 3D torus, but at the moment we can only create 1
allocator: lustre
lustre:
  lq_threshold_rr: 43     # lustre default
  lq_prio_free: 232       # lustre default
  max_nb_ost: 2000        # lustre limit
  stripe_size: 268435456  # Default size of each read / write before switching to another stripe on a different OST (from https://www.alcf.anl.gov/sites/default/files/2022-08/CompPerfWorkshopq_May2022_IOopt_mcpheeters.pdf)
  max_chunks_per_ost: 22
  stripe_count: 1
  stripe_count_high_read_add: 2
  stripe_count_high_thresh_read: 231004494
  stripe_count_high_thresh_write: 264290989
  stripe_count_high_write_add: 2
outputs:
  job_filename_prefix: simulatedJobs_
  io_actions_prefix: io_actions_ts_
  storage_svc_prefix: storage_services_operations_
...
